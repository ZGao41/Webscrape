{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZGao41/Webscrape/blob/main/MMAI5400_Assignment1_Webscrap.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovUjAQZ99Xze"
      },
      "source": [
        "## The ```requests``` module\n",
        "\n",
        "We first demonstrate how to use the Python module ```requests``` to send out ```GET``` request to a web server. We can use it to make API calls as well as to download the ```html``` code for a web page. Here, we will use it for the latter. The ```requests``` module is part of a standard Python installation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2yV-8Zj9Xzi"
      },
      "source": [
        "## Web scraping\n",
        "\n",
        "There is an almost infinite amout of webpages written in ```HTML```, providing a great source of unstructured, textual data for NLP. Fetching html pages is trivial. For example, to get the FAQ page at https://moodle.yorku.ca/students/faq/index.html, we just need to use ```GET``` method of requests module as usual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhe4XT299Xzj"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "resp = requests.get('https://moodle.yorku.ca/students/faq/index.html')\n",
        "from bs4 import BeautifulSoup\n",
        "soup = BeautifulSoup(resp.text, 'html.parser')\n",
        "print(soup.title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMDCoR149Xzk"
      },
      "source": [
        "The difficult part is how to extract useful information out of the fetched html files. ```HTML``` tags could be used to facilitate the extraction. You will need to study a few target html pages, locate the useful information, and recognize the patterns of tags immediately before and after the useful information, and write your Python program to extract the information. Fortunately, there are also Python modules to help you scraping the html pages, for example, ```BeautifulSoup``` and ```Scrapy```. Interested readers shall follow the manuals of these Python modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SE1EpZ-I9Xzk"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# get the htlm\n",
        "resp = requests.get('https://www.ola.org/en/members/current/ministers')\n",
        "\n",
        "# check what the response contains\n",
        "type(resp)\n",
        "print(dir(resp))\n",
        "\n",
        "print(resp.status_code)\n",
        "# HTTPS status codes:\n",
        "# https://en.wikipedia.org/wiki/List_of_HTTP_status_codes\n",
        "\n",
        "print(resp.text)\n",
        "# This is the html source\n",
        "html_code = resp.text\n",
        "\n",
        "# save it so that it can be viewed in an editor\n",
        "with open('resp.html', 'w') as f:\n",
        "    f.write(resp.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Y0lD7mI9Xzl"
      },
      "source": [
        "Open the saved file in a text editor and take a look at the html code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "golq_ul-9Xzl"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "soup = BeautifulSoup(html_code)\n",
        "\n",
        "help(soup)\n",
        "\n",
        "help(soup.find_all)\n",
        "\n",
        "# Find all links\n",
        "links = soup.find_all('a')\n",
        "for link in links:\n",
        "    print(link)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQN3iAH09Xzm"
      },
      "source": [
        "Open the htlm with a text editor and search for example \"Calandra\". You should see that all member names are as link attributes in table cells ```<td>```\n",
        "\n",
        "**Task**:\n",
        "Use ```soup.find_all()``` to find all table cells, and within those cells, print all links.\n",
        "\n",
        "**Question 1**: Does this extract all member names?\n",
        "\n",
        "**Question 2**: Does this extract something that we do not want?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwwzA7sr9Xzm"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9NuUlZK9Xzn"
      },
      "source": [
        "**Task**: Save name and link to a csv file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUy9gEU59Xzn"
      },
      "outputs": [],
      "source": [
        "# open a new file as follows:\n",
        "f = open(\"LAO_members.csv\", 'w')\n",
        "# close it with f.close()\n",
        "# start a csv writer object:\n",
        "writer = csv.writer(f, delimiter=',')\n",
        "# write a row, this writes the header\n",
        "writer.writerow([\"Name\", \"Link\"])\n",
        "\n",
        "# you can extract the text node with link.contents\n",
        "# and you can get the actual link href with link.get('href')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "MMAI5400_class01_WebScraping.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}